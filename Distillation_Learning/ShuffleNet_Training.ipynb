{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a68d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_utils import *\n",
    "from shufflenetV2_tf2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b60bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deac32fd",
   "metadata": {},
   "source": [
    "### DataLoader and other nesseccary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fb85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.04\n",
    "    return lr * np.exp(-k*epoch)\n",
    "\n",
    "def load_batch(paths, main_path=f'{root_path}Data/splitted/train/'):\n",
    "    images = []\n",
    "    feats = []\n",
    "    for i, path in enumerate(paths):\n",
    "        try:\n",
    "            img = cv2.imread(path, cv2.COLOR_BGR2RGB)\n",
    "            img = np.array(img)\n",
    "            img = img.astype('float32')\n",
    "            img /= 255 \n",
    "            images.append(img)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        path_to_feat = path.split('.')[0].split('/')[-1] + '.npy'\n",
    "        path_to_feat = main_path + 'features/mg/original/' + path_to_feat \n",
    "        feats.append(np.load(path_to_feat))\n",
    "    \n",
    "    images = np.array(images)\n",
    "    feats = np.squeeze(np.array(feats))\n",
    "\n",
    "    return images, feats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd814f3b",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e096aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data, data_val,batch_size=128, epochs=30, learning_rate=0.03):\n",
    "    X_train, y_train = load_batch(data)\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=1,\n",
    "                        verbose=0,\n",
    "                        validation_data = data_val)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d894e5c7",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee2f61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = main_path=f'{root_path}Data//splitted/train/'\n",
    "paths_bad = []\n",
    "paths_good = []\n",
    "    \n",
    "for i in range(7):\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    \n",
    "    for data in bad_data:\n",
    "        path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "        paths_bad.append(path_to_img)\n",
    "        \n",
    "alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "good_data = json.load(alm_train_good)\n",
    "for data in good_data:\n",
    "    path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "    paths_good.append(path_to_img)\n",
    "    \n",
    "for i in range(7):\n",
    "    paths_bad[i] = np.squeeze(np.array(paths_bad[i]))\n",
    "paths_good = np.squeeze(np.array(paths_good))\n",
    "   \n",
    "# Generating static validation data\n",
    "paths_bad, paths_bad_val = extract_static_val_data(paths_bad, perc = 0.0017) #original - 0.017\n",
    "paths_good, paths_good_val = extract_static_val_data(paths_good, perc = 0.01) #original - 0.11\n",
    "\n",
    "paths_bad = np.array(paths_bad)\n",
    "paths_bad_val = np.array(paths_bad_val)\n",
    "paths_good = np.array(paths_good)\n",
    "paths_good_val = np.array(paths_good_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae74d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = np.concatenate((np.repeat(paths_good, 7), paths_bad))\n",
    "    \n",
    "#shuffling\n",
    "idx = np.random.permutation(len(full_data))\n",
    "full_data = full_data[idx]\n",
    "full_data = full_data[:10000]\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17ebb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data \n",
    "split_factor = 200\n",
    "splitted_data = []\n",
    "\n",
    "global_batches = int(full_data.shape[0] / split_factor)\n",
    "for i in range(global_batches):\n",
    "    batch_data = full_data[i*split_factor: (i+1)*split_factor]\n",
    "    splitted_data.append(batch_data)\n",
    "    \n",
    "splitted_data[-1] = np.concatenate((splitted_data[-1], full_data[len(splitted_data)*split_factor:]))\n",
    "\n",
    "data = splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35a2931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3181de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading validation data\n",
    "paths_val = np.concatenate( (paths_bad_val, paths_good_val ) , axis=0 )\n",
    "X_val, y_val = load_batch(paths_val)\n",
    "\n",
    "data_val = (X_val, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb97b523",
   "metadata": {},
   "source": [
    "### Creating model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0455249",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cbc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShufflenetV2(num_classes=16928, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f60a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"shufflenet_v2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  672       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  multiple                 96        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     multiple                  0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " shuffle_block (ShuffleBlock  multiple                 4668      \n",
      " )                                                               \n",
      "                                                                 \n",
      " shuffle_block_1 (ShuffleBlo  multiple                 10710     \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " shuffle_block_2 (ShuffleBlo  multiple                 8364      \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (G  multiple                 0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  4350496   \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          multiple                  11008     \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  multiple                 1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_36 (Activation)  multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,387,038\n",
      "Trainable params: 4,384,318\n",
      "Non-trainable params: 2,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build([64, 600, 600, 3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e379b50",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: \n",
      "---------- 0 ----------\n",
      "---------- 1 ----------\n",
      "---------- 2 ----------\n",
      "---------- 3 ----------\n",
      "---------- 4 ----------\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m         batch_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data[i])\n\u001b[1;32m---> 12\u001b[0m         history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     learning_rate = lr_exp_decay(epoch+1, learning_rate)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/ShuffleNet/Shufflenet_on_600x600_labels_MG_original.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, data, data_val, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     38\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m load_batch(data)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#     print(X_train.dtype)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#     print(y_train.dtype)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#     print(data_val[0].dtype)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#     print(data_val[1].dtype)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#     lrs = keras.callbacks.ReduceLROnPlateau()\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;43;03m#                         callbacks=[lrs],\u001b[39;49;00m\n\u001b[0;32m     50\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "                                              epsilon=1e-07, decay=0, amsgrad=False), run_eagerly=True)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}: ')\n",
    "#     if epoch != 0:\n",
    "#         model.load_weights(f'models/ShuffleNet/Shufflenet_on_600x600_labels_MG_original.h5')\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        print(10*'-', i, 10*'-')\n",
    "        batch_data = np.array(data[i])\n",
    "        history = trainer(model, \n",
    "                          batch_data, \n",
    "                          data_val,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs, \n",
    "                          learning_rate=learning_rate)\n",
    "#     learning_rate = lr_exp_decay(epoch+1, learning_rate)\n",
    "    \n",
    "    model.save_weights(f'models/ShuffleNet/Shufflenet_on_600x600_labels_MG_original.h5', save_format='h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c34982f2",
   "metadata": {},
   "source": [
    "### Training on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = tf.random.uniform((64, 600, 600, 3))\n",
    "labels = np.random.randint(0,1,(64, 16928))\n",
    "data = img_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = tf.random.uniform((4, 600, 600, 3))\n",
    "labels = np.random.randint(0,1,(4, 16928))\n",
    "data_val = img_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6295726",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer(model, \n",
    "                  data, \n",
    "                  weights_path,\n",
    "                  data_val,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs, \n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da50bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
