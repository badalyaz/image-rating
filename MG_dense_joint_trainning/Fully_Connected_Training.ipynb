{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4cd1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_utils import *\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220f0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_CE_KLD = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf9f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3e80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "indxs = np.load('variant1_label_9744.npy')\n",
    "def take_from_vector(feature_vector, indxs ):    \n",
    "#     return feature_vector[indxs]\n",
    "    return np.take(feature_vector, indxs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b6b0d80",
   "metadata": {},
   "source": [
    "### Loading alm data features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a282a08",
   "metadata": {},
   "source": [
    "#### Loading  multigap features from .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b156ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_path = root_path + 'Data/splitted/train/'\n",
    "# features_bad_list = []\n",
    "# features_bad_list_i = []\n",
    "# features_good1 = []\n",
    "# feats = 'original_PCA_8464_auto'\n",
    "    \n",
    "# for i in range(7):\n",
    "#     alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "#     bad_data = json.load(alm_train_bad)\n",
    "#     for data in bad_data:\n",
    "#         feat_path = main_path + f'features/mg/{feats}/' + data['feature']\n",
    "#         features_bad_list_i.append(np.load(feat_path))\n",
    "#     features_bad_list.append(features_bad_list_i)\n",
    "#     features_bad_list_i = []\n",
    "        \n",
    "# alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "# good_data = json.load(alm_train_good)\n",
    "# for data in good_data:\n",
    "#     feat_path = main_path + f'features/mg/{feats}/' + data['feature']\n",
    "#     features_good1.append(np.load(feat_path))\n",
    "    \n",
    "# for i in range(7):\n",
    "#     features_bad_list[i] = np.squeeze(np.array(features_bad_list[i]))\n",
    "# features_good1 = np.squeeze(np.array(features_good1))\n",
    "   \n",
    "# # Generating static validation data\n",
    "# features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "# features_good1, features_good1_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "\n",
    "# bad = features_bad_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e9e9f2a",
   "metadata": {},
   "source": [
    "#### Loading cnn features + multigap features from .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f95f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = root_path + 'Data/splitted/train/'\n",
    "features_bad_list = []\n",
    "features_bad_list_i = []\n",
    "features_good1 = []\n",
    "feats_MG = 'original' \n",
    "feats_CNN = 'border_600x600'\n",
    "# feats_CNN_MG_PCA = 'cnn_mg_concat/pca_9744_auto'\n",
    "cnn = 'cnn_efficientnet_b7'\n",
    "    \n",
    "for i in range(7):\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    for data in bad_data:\n",
    "        feat_path_1 = main_path + f'features/mg/{feats_MG}/' + data['feature']\n",
    "        feat_path_2 = main_path + f'features/{cnn}/{feats_CNN}/' + data['feature']\n",
    "        connected = np.concatenate((np.squeeze(np.load(feat_path_1)), np.squeeze(np.load(feat_path_2))))\n",
    "        connected = take_from_vector(connected, indxs) # must be commented later\n",
    "#         connected = main_path + f'features/{feats_CNN_MG_PCA}/' + data['feature']\n",
    "#         connected = np.squeeze(np.load(connected))\n",
    "        features_bad_list_i.append(connected)\n",
    "        \n",
    "    features_bad_list.append(features_bad_list_i)\n",
    "    features_bad_list_i = []\n",
    "      \n",
    "alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "good_data = json.load(alm_train_good)\n",
    "for data in good_data:\n",
    "    feat_path_1 = main_path + f'features/mg/{feats_MG}/' + data['feature']\n",
    "    feat_path_2 = main_path + f'features/{cnn}/{feats_CNN}/' + data['feature']\n",
    "    connected = np.concatenate((np.squeeze(np.load(feat_path_1)), np.squeeze(np.load((feat_path_2)))))\n",
    "    connected = take_from_vector(connected, indxs) # must be commented later\n",
    "#     connected = main_path + f'features/{feats_CNN_MG_PCA}/' + data['feature']\n",
    "#     connected = np.squeeze(np.load(connected))\n",
    "    features_good1.append(connected)\n",
    "    \n",
    "for i in range(7):\n",
    "    features_bad_list[i] = np.squeeze(np.array(features_bad_list[i]))\n",
    "features_good1 = np.squeeze(np.array(features_good1))\n",
    "   \n",
    "# Generating static validation data\n",
    "features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "features_good1, features_good1_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "\n",
    "bad = features_bad_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3264d986",
   "metadata": {},
   "source": [
    "### Creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7f166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.concatenate( (features_bad1_val, features_good1_val ) , axis=0 )\n",
    "y_val = np.concatenate( (np.zeros(len(features_bad1_val)), np.ones(len(features_good1_val)) ), axis=0 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45acec56",
   "metadata": {},
   "source": [
    "### Some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7247a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_graphs(history):\n",
    "    plt.plot(np.arange(2, len(history['loss'])+1), history['loss'][1:])\n",
    "    plt.plot(np.arange(2, len(history['loss'])+1), history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def save_history(history, name):\n",
    "    np.savez(name, val_loss=np.array(history.history['val_loss']),\n",
    "                       loss=np.array(history.history['loss']))\n",
    "    \n",
    "    \n",
    "def calc_acc(model, weights_path, X_test, y_test, batch_size):\n",
    "    '''\n",
    "    Compares Max classes with targets, getting mean class precision\n",
    "    '''\n",
    "    model.load_weights(weights_path)\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    acc = (np.argmax(y_pred,axis=-1) == y_test).mean()\n",
    "    return acc\n",
    "\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.048\n",
    "    return lr * np.exp(-k*epoch)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7233bbc9",
   "metadata": {},
   "source": [
    "### Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44582a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainer(model, data, weights_path, data_val,batch_size=128, epochs=30, learning_rate=0.03):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    model.compile(loss=SC_CE_KLD,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "                                                  epsilon=1e-07, decay=0, amsgrad=False))\n",
    "    model.load_weights(weights_path) \n",
    "\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(weights_path, \n",
    "                                                 monitor='val_loss', \n",
    "                                                 verbose=1, \n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    \n",
    "    \n",
    "    schedule = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=0)\n",
    "    callbacks_list = [checkpoint, schedule]\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data = data_val)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d443d1bf",
   "metadata": {},
   "source": [
    "### Creating and loading weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a38391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fc_model_softmax(input_num=9744)\n",
    "weights_path_pretrained = f'models/Softmax/MG_CNN/model_fc_softmax_v1_9744_label.hdf5'\n",
    "# model.save_weights(weights_path_pretrained) #if we want to cancel learning and start from 0\n",
    "model.load_weights(weights_path_pretrained)\n",
    "weights_path =  f'models/Softmax/MG_CNN/model_fc_softmax_v1_9744_label.hdf5'     \n",
    "# model.save_weights(weights_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5d42d9a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e05635a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c0293f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8899, 9744)\n",
      "(10000, 9744)\n",
      "(10000, 9744)\n",
      "(9999, 9744)\n",
      "(9998, 9744)\n",
      "(9997, 9744)\n",
      "(10717, 9744)\n",
      "(10465, 9744)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bad)):\n",
    "    print(bad[i].shape)\n",
    "print(features_good1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68ecc69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_bad shape (10717, 9744)\n",
      "train_data_good shape (10465, 9744)\n",
      "Epoch 1/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0720\n",
      "Epoch 1: val_loss improved from inf to 0.07486, saving model to models/Softmax/MultiGap_CNN\\model_fc_softmax_v1_9744_label.hdf5\n",
      "158/158 [==============================] - 3s 17ms/step - loss: 0.0720 - val_loss: 0.0749 - lr: 0.0030\n",
      "Epoch 2/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0600\n",
      "Epoch 2: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0600 - val_loss: 0.0832 - lr: 0.0029\n",
      "Epoch 3/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0500\n",
      "Epoch 3: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0499 - val_loss: 0.0798 - lr: 0.0026\n",
      "Epoch 4/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0436\n",
      "Epoch 4: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 15ms/step - loss: 0.0436 - val_loss: 0.0853 - lr: 0.0022\n",
      "Epoch 5/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0369\n",
      "Epoch 5: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 15ms/step - loss: 0.0372 - val_loss: 0.0786 - lr: 0.0019\n",
      "Epoch 6/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0301\n",
      "Epoch 6: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0301 - val_loss: 0.0930 - lr: 0.0015\n",
      "Epoch 7/15\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.0223\n",
      "Epoch 7: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0223 - val_loss: 0.0865 - lr: 0.0011\n",
      "Epoch 8/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0211\n",
      "Epoch 8: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0214 - val_loss: 0.0789 - lr: 7.8240e-04\n",
      "Epoch 9/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0179\n",
      "Epoch 9: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0179 - val_loss: 0.0823 - lr: 5.3292e-04\n",
      "Epoch 10/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0126\n",
      "Epoch 10: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0126 - val_loss: 0.0839 - lr: 3.4598e-04\n",
      "Epoch 11/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0110\n",
      "Epoch 11: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0110 - val_loss: 0.0834 - lr: 2.1408e-04\n",
      "Epoch 12/15\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 12: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 15ms/step - loss: 0.0110 - val_loss: 0.0867 - lr: 1.2626e-04\n",
      "Epoch 13/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0104\n",
      "Epoch 13: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0104 - val_loss: 0.0880 - lr: 7.0978e-05\n",
      "Epoch 14/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Epoch 14: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0075 - val_loss: 0.0889 - lr: 3.8030e-05\n",
      "Epoch 15/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.0081\n",
      "Epoch 15: val_loss did not improve from 0.07486\n",
      "158/158 [==============================] - 2s 14ms/step - loss: 0.0090 - val_loss: 0.0887 - lr: 1.9421e-05\n",
      "----- Accuracy = 0.9749163879598662  -----\n",
      "---Batch Train Done---\n"
     ]
    }
   ],
   "source": [
    "data_val = (X_val, y_val)\n",
    "\n",
    "i = 6\n",
    "data = get_train_pairs(bad[i], features_good1, train_size=0.95, shuffle=True)\n",
    "if True:\n",
    "    history = trainer(model, data, weights_path, data_val, batch_size, epochs, learning_rate=learning_rate)\n",
    "    acc = calc_acc(model, weights_path, data_val[0], data_val[1], batch_size)\n",
    "\n",
    "    #save_history(history, f'histories/denoised_good_to_bad/model_fc_good_denoised_to_bad_02_08_bad{i+1}')\n",
    "\n",
    "    print('----- Accuracy =', acc, ' -----')\n",
    "    print('---Batch Train Done---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c3d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
